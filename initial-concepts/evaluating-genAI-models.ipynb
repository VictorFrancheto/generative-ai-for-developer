{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9bc598",
   "metadata": {},
   "source": [
    "# üéØ Evaluation Techniques for Generative AI Models\n",
    "\n",
    "Welcome to this comprehensive guide on evaluating Generative AI model outputs! This notebook demonstrates several essential techniques used to measure the quality and accuracy of AI-generated content.\n",
    "\n",
    "## üìö What You'll Learn\n",
    "- **Exact Match**: Strict comparison for precise answers\n",
    "- **ROUGE**: Lexical similarity for text evaluation\n",
    "- **Semantic Similarity**: Meaning-based comparison using embeddings\n",
    "- **Functional Correctness**: Code validation through unit tests\n",
    "- **Pass@k**: Multiple attempt success rate\n",
    "- **LLM-as-a-Judge**: AI-powered subjective evaluation\n",
    "\n",
    "## üöÄ Getting Started\n",
    "\n",
    "> **üìù Note:** First, we'll import the necessary libraries including tools for data handling, mathematical operations, and specialized evaluation metrics. Setting a random seed ensures reproducible results across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65301ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324bca82",
   "metadata": {},
   "source": [
    "## üéØ Exact Match (EM)\n",
    "\n",
    "### Overview\n",
    "Exact Match is the **simplest and strictest** evaluation metric available. It verifies whether the model's output is **perfectly identical** to the reference answer after normalization.\n",
    "\n",
    "### When to Use\n",
    "- ‚úÖ Multiple-choice questions\n",
    "- ‚úÖ Tasks with single, clear correct answers\n",
    "- ‚úÖ Classification tasks with specific labels\n",
    "- ‚ùå Open-ended text generation\n",
    "- ‚ùå Creative writing tasks\n",
    "\n",
    "### How It Works\n",
    "The metric normalizes both strings (lowercase, trim whitespace) and returns 1 for a perfect match, 0 otherwise.\n",
    "\n",
    "> **üìù Note:** In this example, we compare predicted fruit names against correct labels. We define a simple normalize function to make text lowercase and remove extra whitespace before comparing. The final score is the average of all individual comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6281321f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Scores: [1, 1, 0]\n",
      "Average Exact Match Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Let's compare predicted fruit names with the correct labels.\n",
    "preds = [\"Apple\", \"banana \", \" Orange\"]\n",
    "labels = [\"apple\", \"banana\", \"grape\"]\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    \"\"\"Normalize a string by lowercasing and stripping whitespace.\"\"\"\n",
    "    return s.lower().strip()\n",
    "\n",
    "def exact_match(pred: str, label: str) -> int:\n",
    "    \"\"\"Return 1 if normalized strings are identical, else 0.\"\"\"\n",
    "    return int(normalize(pred) == normalize(label))\n",
    "\n",
    "# Calculate EM score for each pair\n",
    "em_scores = [exact_match(p, l) for p, l in zip(preds, labels)]\n",
    "\n",
    "# The final score is the average of individual scores\n",
    "em_accuracy = sum(em_scores) / len(em_scores)\n",
    "\n",
    "print(f\"Individual Scores: {em_scores}\")\n",
    "print(f\"Average Exact Match Accuracy: {em_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f27642",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "As shown in the output, **two out of three** predictions match perfectly after normalization, resulting in an average accuracy of approximately **67%**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Lexical Similarity (ROUGE)\n",
    "\n",
    "### Overview\n",
    "\n",
    "**ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** is a flexible evaluation metric that measures the overlap of words or *n-grams* between a model‚Äôs prediction and a reference text.\n",
    "\n",
    "An **n-gram** is a sequence of consecutive words:\n",
    "- **Unigram (n = 1):** single words  \n",
    "- **Bigram (n = 2):** pairs of consecutive words  \n",
    "- **Trigram (n = 3):** sequences of three consecutive words  \n",
    "\n",
    "The term **overlap** refers to how many of these words or word sequences appear in both the generated text and the reference text.\n",
    "\n",
    "In practice:\n",
    "- **High overlap** ‚Üí the generated text uses similar words and phrases as the reference.  \n",
    "- **Low overlap** ‚Üí the generated text differs significantly in wording or structure.\n",
    "\n",
    "ROUGE is commonly used to evaluate tasks such as **text summarization and text generation**, where lexical similarity to a reference output is important.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Example\n",
    "\n",
    "Reference text:\n",
    "> **\"the cat sits on the mat\"**\n",
    "\n",
    "Generated text:\n",
    "> **\"the cat sits on mat\"**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Unigrams (n = 1)\n",
    "\n",
    "Reference unigrams:\n",
    "```\n",
    "[the, cat, sits, on, the, mat]\n",
    "```\n",
    "\n",
    "Generated unigrams:\n",
    "```\n",
    "[the, cat, sits, on, mat]\n",
    "```\n",
    "\n",
    "Overlap:\n",
    "```\n",
    "[the, cat, sits, on, mat]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Bigrams (n = 2)\n",
    "\n",
    "Reference bigrams:\n",
    "```\n",
    "[the cat, cat sits, sits on, on the, the mat]\n",
    "```\n",
    "\n",
    "Generated bigrams:\n",
    "```\n",
    "[the cat, cat sits, sits on, on mat]\n",
    "```\n",
    "\n",
    "Overlap:\n",
    "```\n",
    "[the cat, cat sits, sits on]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Trigrams (n = 3)\n",
    "\n",
    "Reference trigrams:\n",
    "```\n",
    "[the cat sits, cat sits on, sits on the, on the mat]\n",
    "```\n",
    "\n",
    "Generated trigrams:\n",
    "```\n",
    "[the cat sits, cat sits on, sits on mat]\n",
    "```\n",
    "\n",
    "Overlap:\n",
    "```\n",
    "[the cat sits, cat sits on]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Interpretation\n",
    "\n",
    "- **ROUGE-1** measures overlap of single words (unigrams).  \n",
    "- **ROUGE-2** measures overlap of word pairs (bigrams).  \n",
    "- **ROUGE-3** measures overlap of word triples (trigrams). \n",
    "- **ROUGE-L** measures how much the word order and structure of the generated text match the reference text, using the Longest Common Subsequence (LCS). \n",
    "- More overlap indicates higher lexical similarity between the generated text and the reference.\n",
    "\n",
    "---\n",
    "\n",
    "## üìê Mathematical Definition of ROUGE-L\n",
    "\n",
    "Let:\n",
    "\n",
    "- $X$ be the reference text.  \n",
    "- $Y$ be the generated (predicted) text.  \n",
    "- $\\mathrm{LCS}(X, Y)$ be the length of the **Longest Common Subsequence** between $X$ and $Y$.\n",
    "\n",
    "We define:\n",
    "\n",
    "### ‚úÖ Recall\n",
    "\n",
    "$$\n",
    "R_{\\mathrm{LCS}} = \\frac{\\mathrm{LCS}(X, Y)}{|X|}\n",
    "$$\n",
    "\n",
    "### ‚úÖ Precision\n",
    "\n",
    "$$\n",
    "P_{\\mathrm{LCS}} = \\frac{\\mathrm{LCS}(X, Y)}{|Y|}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $|X|$ denotes the number of tokens (words) in the reference text.  \n",
    "- $|Y|$ denotes the number of tokens (words) in the generated text.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ F1 Score (ROUGE-L)\n",
    "\n",
    "The ROUGE-L score is computed as the F1 measure based on precision and recall:\n",
    "\n",
    "$$\n",
    "F_{\\mathrm{LCS}} =\n",
    "\\frac{(1 + \\beta^2) \\cdot P_{\\mathrm{LCS}} \\cdot R_{\\mathrm{LCS}}}\n",
    "{R_{\\mathrm{LCS}} + \\beta^2 \\cdot P_{\\mathrm{LCS}}}\n",
    "$$\n",
    "\n",
    "Typically, $\\beta = 1$, which simplifies the expression to:\n",
    "\n",
    "$$\n",
    "F_{\\mathrm{LCS}} =\n",
    "\\frac{2 \\cdot P_{\\mathrm{LCS}} \\cdot R_{\\mathrm{LCS}}}\n",
    "{P_{\\mathrm{LCS}} + R_{\\mathrm{LCS}}}\n",
    "$$\n",
    "\n",
    "This value is commonly reported as the **ROUGE-L score**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Limitations\n",
    "\n",
    "ROUGE focuses on **lexical similarity**, not semantic meaning.  \n",
    "Two sentences can express the same idea using different words and still receive a low ROUGE score.\n",
    "\n",
    "\n",
    "### When to Use\n",
    "- ‚úÖ Text summarization\n",
    "- ‚úÖ Machine translation\n",
    "- ‚úÖ Tasks where answers can be phrased differently\n",
    "- ‚úÖ Content paraphrasing evaluation\n",
    "\n",
    "\n",
    "Higher scores (closer to 1.0) indicate better lexical similarity.\n",
    "\n",
    "> **üìù Note:** Here we compare two sentences with similar words but different structures using the `evaluate` library. The ROUGE metric provides multiple scores to capture different aspects of text overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "698a9cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Score: 0.8000\n",
      "ROUGE-L Score: 0.6000\n"
     ]
    }
   ],
   "source": [
    "# Define a prediction and a reference text\n",
    "pred = \"the quick brown fox\"\n",
    "label = \"the fox is quick and brown\"\n",
    "\n",
    "# Load the ROUGE metric from the 'evaluate' library\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Compute the scores\n",
    "results = rouge.compute(predictions=[pred], references=[label])\n",
    "\n",
    "print(f\"ROUGE-1 Score: {results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-L Score: {results['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601132c",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "The high scores indicate **strong lexical similarity** between the two sentences, even though they have different word orders and structures.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Semantic Similarity\n",
    "\n",
    "### üìå Overview\n",
    "\n",
    "Semantic similarity measures how close two texts are in **meaning**, rather than in exact word overlap.  \n",
    "Each sentence is mapped to a numerical vector (embedding) in a high-dimensional space, and similarity is quantified using a vector similarity metric, typically **cosine similarity**.\n",
    "\n",
    "Formally, let:\n",
    "\n",
    "- $x \\in \\mathbb{R}^d$ be the embedding vector of sentence $S_1$.  \n",
    "- $y \\in \\mathbb{R}^d$ be the embedding vector of sentence $S_2$.  \n",
    "\n",
    "The semantic similarity between $S_1$ and $S_2$ is defined as the cosine of the angle between $x$ and $y$.\n",
    "\n",
    "---\n",
    "\n",
    "### üìê Cosine Similarity\n",
    "\n",
    "The cosine similarity is given by:\n",
    "\n",
    "$$\n",
    "\\mathrm{sim}(x, y) = \n",
    "\\frac{x \\cdot y}{\\|x\\|_2 \\, \\|y\\|_2}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $x \\cdot y = \\sum_{i=1}^{d} x_i y_i$ is the dot product,  \n",
    "- $\\|x\\|_2 = \\sqrt{\\sum_{i=1}^{d} x_i^2}$ is the Euclidean norm.\n",
    "\n",
    "The similarity score satisfies:\n",
    "\n",
    "$$\n",
    "-1 \\leq \\mathrm{sim}(x, y) \\leq 1\n",
    "$$\n",
    "\n",
    "- $\\mathrm{sim}(x, y) \\approx 1$ ‚Üí very similar semantic meaning  \n",
    "- $\\mathrm{sim}(x, y) \\approx 0$ ‚Üí weak or no semantic relation  \n",
    "- $\\mathrm{sim}(x, y) \\approx -1$ ‚Üí opposite semantic direction (rare in practice for embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Computational Pipeline\n",
    "\n",
    "1. **Encoding**  \n",
    "   A pre-trained embedding model $f(\\cdot)$ maps each sentence to a vector:\n",
    "   $$\n",
    "   x = f(S_1), \\quad y = f(S_2)\n",
    "   $$\n",
    "\n",
    "2. **Similarity Computation**  \n",
    "   Compute the cosine similarity between the embeddings:\n",
    "   $$\n",
    "   s = \\mathrm{sim}(x, y)\n",
    "   $$\n",
    "\n",
    "3. **Interpretation**  \n",
    "   The scalar score $s$ quantifies semantic proximity between the two sentences.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ When to Use\n",
    "\n",
    "Semantic similarity is commonly applied in:\n",
    "\n",
    "- ‚úÖ Paraphrase detection  \n",
    "- ‚úÖ Question‚Äìanswer matching  \n",
    "- ‚úÖ Duplicate content detection  \n",
    "- ‚úÖ Semantic search and information retrieval  \n",
    "- ‚úÖ Clustering and recommendation systems\n",
    "\n",
    "\n",
    "> **üìù Note:** We'll load the `all-MiniLM-L6-v2` model, which is optimized for creating sentence embeddings. Then we'll generate embeddings for predictions and labels, calculating cosine similarity for each pair to see how semantically related they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d92bd1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1:\n",
      "  Pred:  'Dogs make great companions'\n",
      "  Label: 'A dog is a loyal pet'\n",
      "  Similarity: 0.6147\n",
      "\n",
      "Pair 2:\n",
      "  Pred:  'A cat is a solitary creature'\n",
      "  Label: 'Cats are independent animals'\n",
      "  Similarity: 0.6848\n",
      "\n",
      "Pair 3:\n",
      "  Pred:  'The ocean is vast'\n",
      "  Label: 'The sky is blue'\n",
      "  Similarity: 0.3098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Define prediction and label sentences\n",
    "labels = [\"A dog is a loyal pet\", \"Cats are independent animals\", \"The sky is blue\"]\n",
    "preds = [\n",
    "    \"Dogs make great companions\",\n",
    "    \"A cat is a solitary creature\",\n",
    "    \"The ocean is vast\",\n",
    "]\n",
    "\n",
    "# 3. Generate embeddings for each list\n",
    "pred_embeddings = model.encode(preds)\n",
    "label_embeddings = model.encode(labels)\n",
    "\n",
    "# 4. Calculate cosine similarity for each pair\n",
    "for i in range(len(preds)):\n",
    "    similarity = np.dot(pred_embeddings[i], label_embeddings[i]) / (\n",
    "        np.linalg.norm(pred_embeddings[i]) * np.linalg.norm(label_embeddings[i])\n",
    "    )\n",
    "    print(\n",
    "        f\"Pair {i + 1}:\\n  Pred:  '{preds[i]}'\\n  Label: '{labels[i]}'\\n  Similarity: {similarity:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37ffa4",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "Notice that sentences about **cats and dogs** have high similarity scores because their meanings are semantically related. In contrast, sentences about **ocean and sky** have low scores despite both being nature-related, because they describe fundamentally different concepts.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Functional Correctness\n",
    "\n",
    "### Overview\n",
    "For code generation tasks, we need to verify if the **generated code actually works**. Functional Correctness evaluates code by running it against a suite of unit tests.\n",
    "\n",
    "### When to Use\n",
    "- ‚úÖ Code generation evaluation\n",
    "- ‚úÖ Programming assistance tools\n",
    "- ‚úÖ Automated coding challenges\n",
    "- ‚úÖ Algorithm implementation verification\n",
    "\n",
    "### How It Works\n",
    "1. Generate code with the model\n",
    "2. Run the code against predefined test cases\n",
    "3. Calculate the proportion of tests that pass\n",
    "4. Score = (Passed Tests / Total Tests)\n",
    "\n",
    "> **üìù Note:** In this example, we test a function that reverses and capitalizes strings. However, it contains a bug: it fails when the input contains digits. We'll run three test cases to demonstrate how functional correctness catches these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11deef1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'hello' -> Output: 'OLLEH', Expected: 'OLLEH'\n",
      "Input: 'world1' -> Output: 'ERROR - CONTAINS DIGITS', Expected: '1DLROW'\n",
      "Input: 'python' -> Output: 'NOHTYP', Expected: 'NOHTYP'\n",
      "\n",
      "Proportion of tests passed: 0.67\n"
     ]
    }
   ],
   "source": [
    "# This function is supposed to reverse and capitalize a string,\n",
    "# but it has a bug: it fails if the string contains a number.\n",
    "def reverse_and_capitalize(s: str) -> str:\n",
    "    \"\"\"Reverse and capitalize a string, with a hidden bug.\"\"\"\n",
    "    if any(char.isdigit() for char in s):\n",
    "        return \"ERROR - CONTAINS DIGITS\"\n",
    "    return s[::-1].upper()\n",
    "\n",
    "# Test cases: one prediction will trigger the bug\n",
    "code_preds = [\"hello\", \"world1\", \"python\"]\n",
    "test_labels = [\"OLLEH\", \"1DLROW\", \"NOHTYP\"]\n",
    "\n",
    "# Run the generated code against the test labels\n",
    "results = []\n",
    "for pred_code, label in zip(code_preds, test_labels):\n",
    "    output = reverse_and_capitalize(pred_code)\n",
    "    print(f\"Input: '{pred_code}' -> Output: '{output}', Expected: '{label}'\")\n",
    "    results.append(output == label)\n",
    "\n",
    "pass_rate = sum(results) / len(results)\n",
    "print(f\"\\nProportion of tests passed: {pass_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a5c3a",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "The function works correctly for **\"hello\"** and **\"python\"** but fails for **\"world1\"** due to the digit check bug. As a result, the pass rate is **2 out of 3** (approximately 67%), clearly identifying the function's limitation.\n",
    "\n",
    "---\n",
    "\n",
    "## üé≤ Pass@k\n",
    "\n",
    "### Overview\n",
    "Pass@k evaluates scenarios where a model generates **k multiple attempts** for a single problem. If **at least one** of these attempts is correct, it counts as a success.\n",
    "\n",
    "### When to Use\n",
    "- ‚úÖ Code generation with multiple solutions\n",
    "- ‚úÖ Creative tasks with multiple valid answers\n",
    "- ‚úÖ Brainstorming applications\n",
    "- ‚úÖ When diversity in outputs is encouraged\n",
    "\n",
    "### How It Works\n",
    "- Generate k samples for one problem\n",
    "- Check if any sample matches the correct answer\n",
    "- Return 1 if at least one is correct, 0 otherwise\n",
    "- Common values: Pass@1, Pass@5, Pass@10\n",
    "\n",
    "> **üìù Note:** We'll simulate a scenario where a model generated 4 possible answers when asked to \"name a primary color.\" Our function checks if the correct answer (\"blue\") is present anywhere in the list of generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2421754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: ['red', 'yellow', 'green', 'blue']\n",
      "Label: blue\n",
      "Pass@4 Score: 1\n"
     ]
    }
   ],
   "source": [
    "def pass_at_k(samples: list[str], label: str) -> int:\n",
    "    \"\"\"Return 1 if any sample in the list matches the label, else 0.\"\"\"\n",
    "    return int(any(s == label for s in samples))\n",
    "\n",
    "# The model generated 4 possible answers for \"Name a primary color.\"\n",
    "label = \"blue\"\n",
    "samples = [\"red\", \"yellow\", \"green\", \"blue\"]\n",
    "\n",
    "# Check if any of the 4 samples is correct\n",
    "pass_score = pass_at_k(samples, label)\n",
    "\n",
    "print(f\"Samples: {samples}\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Pass@4 Score: {pass_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e84c81",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "Since the correct answer **\"blue\"** appears in the list of samples, the function returns a score of **1**, indicating a successful Pass@4. This demonstrates that the model succeeded in generating the correct answer within 4 attempts.\n",
    "\n",
    "---\n",
    "\n",
    "## üßë‚Äç‚öñÔ∏è LLM-as-a-Judge\n",
    "\n",
    "### Overview\n",
    "For **complex and subjective** tasks (creativity, helpfulness, tone), we can leverage another powerful LLM to act as an evaluator. The judge LLM receives the prediction, reference answer, and a detailed rubric to provide scored feedback.\n",
    "\n",
    "### When to Use\n",
    "- ‚úÖ Creative writing evaluation\n",
    "- ‚úÖ Subjective quality assessment\n",
    "- ‚úÖ Multi-criteria evaluation\n",
    "- ‚úÖ Tasks without clear right/wrong answers\n",
    "- ‚úÖ Nuanced scoring requirements\n",
    "\n",
    "### How It Works\n",
    "1. Define a clear evaluation rubric\n",
    "2. Provide the judge with: prediction, reference, and rubric\n",
    "3. Judge returns a score with reasoning\n",
    "4. Can scale to multiple criteria and complex scoring\n",
    "\n",
    "> **üìù Note:** We'll define a rubric for scoring animal predictions with three tiers: perfect match (1.0), same biological class (0.5), or different class (0.0). The judge function will evaluate three different test cases demonstrating each scoring scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8017787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAMMALS = {\"lion\", \"tiger\", \"dog\", \"cat\", \"horse\"}\n",
    "REPTILES = {\"snake\", \"lizard\", \"crocodile\"}\n",
    "\n",
    "def llm_as_judge(pred, label, rubric):\n",
    "    pred = pred.lower()\n",
    "    label = label.lower()\n",
    "\n",
    "    if pred == label:\n",
    "        return 1.0\n",
    "\n",
    "    if (pred in MAMMALS and label in MAMMALS) or \\\n",
    "       (pred in REPTILES and label in REPTILES):\n",
    "        return 0.5\n",
    "\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92108c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Final Score: 1.0\n",
      "\n",
      "--> Final Score: 0.5\n",
      "\n",
      "--> Final Score: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is our rubric for the judge.\n",
    "RUBRIC = \"\"\"\n",
    "Score 1.0 if the predicted animal is the same as the label.\n",
    "Score 0.5 if the prediction is a different animal but from the same biological class (e.g., both are mammals).\n",
    "Score 0.0 otherwise (e.g., a mammal and a reptile).\n",
    "\"\"\"\n",
    "\n",
    "# ... A mock function `llm_as_judge` is defined here to simulate an LLM's response ...\n",
    "\n",
    "# --- Test Case 1: Perfect Match ---\n",
    "score1 = llm_as_judge(pred=\"Lion\", label=\"Lion\", rubric=RUBRIC)\n",
    "print(f\"--> Final Score: {score1}\\n\")\n",
    "\n",
    "# --- Test Case 2: Same Class ---\n",
    "score2 = llm_as_judge(pred=\"Tiger\", label=\"Lion\", rubric=RUBRIC)\n",
    "print(f\"--> Final Score: {score2}\\n\")\n",
    "\n",
    "# --- Test Case 3: Different Class ---\n",
    "score3 = llm_as_judge(pred=\"Snake\", label=\"Lion\", rubric=RUBRIC)\n",
    "print(f\"--> Final Score: {score3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f0e3a",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "The LLM-as-a-Judge correctly applies the rubric:\n",
    "- **Test Case 1** (Lion vs Lion): Perfect match ‚Üí Score **1.0** ‚úÖ\n",
    "- **Test Case 2** (Tiger vs Lion): Same biological class (both mammals) ‚Üí Score **0.5** ‚ö°\n",
    "- **Test Case 3** (Snake vs Lion): Different classes (reptile vs mammal) ‚Üí Score **0.0** ‚ùå\n",
    "\n",
    "This demonstrates how an LLM can apply nuanced reasoning to complex evaluation tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Conclusion\n",
    "\n",
    "You've now learned **six powerful techniques** for evaluating Generative AI outputs! Each metric serves different purposes:\n",
    "\n",
    "| Metric | Best For | Strictness |\n",
    "|--------|----------|-----------|\n",
    "| Exact Match | Classification tasks | Highest |\n",
    "| ROUGE | Text summarization | Medium |\n",
    "| Semantic Similarity | Paraphrase detection | Medium |\n",
    "| Functional Correctness | Code generation | High |\n",
    "| Pass@k | Multiple attempts | Low |\n",
    "| LLM-as-a-Judge | Subjective tasks | Customizable |\n",
    "\n",
    "Choose the right metric based on your specific use case! üöÄ\n",
    "\n",
    "---\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
