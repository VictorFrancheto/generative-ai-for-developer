{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "demo-intro-1",
   "metadata": {},
   "source": [
    "# Demo: Teach an LLM a New Skill with SFT\n",
    "\n",
    "Welcome! This notebook is a short demonstration to show you how to teach a Large Language Model (LLM) a new skill using Supervised Fine-Tuning (SFT). \n",
    "\n",
    "LLMs are great at many things, but they don't know everything. Sometimes, we need to teach them a specific, new task. In this demo, we'll teach a small LLM to add the suffish \"-ish\" to the ends of words.\n",
    "\n",
    "This demo follows the exact same structure as the exercise you're about to do. Pay attention to the steps, as you'll be repeating them to teach the model how to spell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-outline-2",
   "metadata": {},
   "source": [
    "## What you'll see in this demo\n",
    "\n",
    "1.  **Setup**: Import libraries and configure the environment.\n",
    "2.  **Load the model**: Use a small, instruction-tuned model as our starting point.\n",
    "3.  **Create a dataset**: Generate a simple dataset of words and their -ish variants.\n",
    "4.  **Evaluate the base model**: See how the model does *before* any training.\n",
    "5.  **Configure LoRA and train**: Use Parameter-Efficient Fine-Tuning (PEFT) with LoRA to train our model efficiently.\n",
    "6.  **Evaluate the fine-tuned model**: Test the model again to see its new skill in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-setup-hdr",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f3051a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Using cached trl-0.28.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trl) (1.12.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trl) (4.5.0)\n",
      "Requirement already satisfied: packaging>20.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trl) (25.0)\n",
      "Requirement already satisfied: transformers>=4.56.2 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trl) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate>=1.4.0->trl) (1.26.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from accelerate>=1.4.0->trl) (7.2.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate>=1.4.0->trl) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate>=1.4.0->trl) (2.10.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate>=1.4.0->trl) (1.4.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate>=1.4.0->trl) (0.7.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=3.0.0->trl) (3.19.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=3.0.0->trl) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=3.0.0->trl) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=3.0.0->trl) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=3.0.0->trl) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from datasets>=3.0.0->trl) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets>=3.0.0->trl) (0.70.18)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2025.10.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (0.23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.15.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.66.3->datasets>=3.0.0->trl) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=4.56.2->trl) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=4.56.2->trl) (0.22.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from anyio->httpx<1.0.0->datasets>=3.0.0->trl) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n",
      "Requirement already satisfied: typer>=0.23.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (0.23.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from typer>=0.23.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (8.2.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.23.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (14.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.23.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\victor.francheto\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\victor.francheto\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.0->typer-slim->huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (0.1.2)\n",
      "Using cached trl-0.28.0-py3-none-any.whl (540 kB)\n",
      "Installing collected packages: trl\n",
      "Successfully installed trl-0.28.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.3 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "demo-setup-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Setup necessary imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# Use GPU, MPS, or CPU, depending on what's available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.set_num_threads(max(1, os.cpu_count() // 2))\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-load-model-hdr",
   "metadata": {},
   "source": [
    "## Step 1. Load the tokenizer and base model\n",
    "\n",
    "We'll use `HuggingFaceTB/SmolLM2-135M-Instruct`, a small model with 135 million parameters. Its small size makes it perfect for a quick demonstration on a standard computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "demo-load-model-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victor.francheto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\victor.francheto\\.cache\\huggingface\\hub\\models--HuggingFaceTB--SmolLM2-135M-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 272/272 [00:00<00:00, 770.32it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: HuggingFaceTB/SmolLM2-135M-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Model ID from Hugging Face\n",
    "model_id = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "# Load the tokenizer, which prepares text for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Load the model itself\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Move the model to our selected device (GPU/CPU)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-create-ds-hdr",
   "metadata": {},
   "source": [
    "## Step 2. Create the dataset\n",
    "\n",
    "Next, we'll create a small dataset of examples to teach the model our new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "demo-word-list",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset will be created from 62 words.\n"
     ]
    }
   ],
   "source": [
    "# A list of words for our demonstration\n",
    "\n",
    "# fmt: off\n",
    "DEMO_WORDS = [\n",
    "    \"idea\", \"glow\", \"rust\", \"maze\", \"echo\", \"wisp\", \"veto\", \"lush\", \"gaze\", \"knit\", \"fume\", \"plow\",\n",
    "    \"void\", \"oath\", \"grim\", \"crisp\", \"lunar\", \"fable\", \"quest\", \"verge\", \"brawn\", \"elude\", \"aisle\",\n",
    "    \"ember\", \"crave\", \"ivory\", \"mirth\", \"knack\", \"wryly\", \"onset\", \"mosaic\", \"velvet\", \"sphinx\",\n",
    "    \"radius\", \"summit\", \"banner\", \"cipher\", \"glisten\", \"mantle\", \"scarab\", \"expose\", \"fathom\",\n",
    "    \"tavern\", \"fusion\", \"relish\", \"lantern\", \"enchant\", \"torrent\", \"capture\", \"orchard\", \"eclipse\",\n",
    "    \"frescos\", \"triumph\", \"absolve\", \"gossipy\", \"prelude\", \"whistle\", \"resolve\", \"zealous\",\n",
    "    \"mirage\", \"aperture\", \"sapphire\",\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "print(f\"Dataset will be created from {len(DEMO_WORDS)} words.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "demo-generate-records",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 62 examples [00:00, 14172.26 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training example:\n",
      "{'prompt': 'Add -ish to the end of the word.\\nhello -> hello-ish\\nlearn -> learn-ish\\nivory -> ', 'completion': 'ivory-ish'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This function creates prompt/completion pairs for our dataset.\n",
    "def generate_records():\n",
    "    for word in DEMO_WORDS:\n",
    "        # The prompt tells the model what to do.\n",
    "        prompt = (\n",
    "            f\"Add -ish to the end of the word.\\n\"\n",
    "            \"hello -> hello-ish\\n\"\n",
    "            \"learn -> learn-ish\\n\"\n",
    "            f\"{word} -> \"\n",
    "        )\n",
    "        # The completion is the correct answer.\n",
    "        completion = f\"{word}-ish\"\n",
    "        yield {\"prompt\": prompt, \"completion\": completion}\n",
    "\n",
    "\n",
    "# Create a Hugging Face Dataset from our generator\n",
    "ds = Dataset.from_generator(generate_records)\n",
    "\n",
    "# Split the dataset: 80% for training, 20% for testing\n",
    "ds = ds.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Let's look at the first training example\n",
    "print(\"First training example:\")\n",
    "print(ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-eval-base-hdr",
   "metadata": {},
   "source": [
    "## Step 3. Evaluate the base model\n",
    "\n",
    "Before we train, let's see if the model already knows how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "demo-check-translation-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to test the model's translation ability\n",
    "def check_translation(model, tokenizer, prompt: str, actual_translation: str):\n",
    "    # Prepare the input for the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate a response from the model\n",
    "    gen = model.generate(**inputs, max_new_tokens=15, use_cache=False)\n",
    "    output = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract just the translated part\n",
    "    proposed_translation = output.split(\"->\")[-1].strip().split(\"\\n\")[0].strip()\n",
    "\n",
    "    # Check if the model's answer is correct\n",
    "    is_correct = proposed_translation == actual_translation\n",
    "    print(\n",
    "        f\"Proposed: {proposed_translation} | Actual: {actual_translation} \"\n",
    "        f\"| Correct: {'âœ…' if is_correct else 'âŒ'}\"\n",
    "    )\n",
    "    return is_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "demo-eval-base-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Base Model (Before Training) ---\n",
      "Proposed: wryly-ish | Actual: wryly-ish | Correct: âœ…\n",
      "Proposed: 1-ish | Actual: glisten-ish | Correct: âŒ\n",
      "Proposed: quest-ish | Actual: quest-ish | Correct: âœ…\n",
      "Proposed: ire-ish | Actual: crave-ish | Correct: âŒ\n",
      "Proposed: ils-ish | Actual: lush-ish | Correct: âŒ\n",
      "Proposed: Ñ„Ð°Ð¹Ð»ÐµÐ¹ | Actual: fable-ish | Correct: âŒ\n",
      "Proposed: knack-ish | Actual: knack-ish | Correct: âœ…\n",
      "Proposed: iumph-ish | Actual: triumph-ish | Correct: âŒ\n",
      "Proposed: sapphire-ish | Actual: sapphire-ish | Correct: âœ…\n",
      "Proposed: expose-ish | Actual: expose-ish | Correct: âœ…\n",
      "Proposed: ils-es | Actual: frescos-ish | Correct: âŒ\n",
      "Proposed: wisp-ish | Actual: wisp-ish | Correct: âœ…\n",
      "Proposed: mi-rage | Actual: mirage-ish | Correct: âŒ\n",
      "\n",
      "Result: 6/13 correct.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Evaluating Base Model (Before Training) ---\")\n",
    "num_correct = 0\n",
    "num_examples = len(ds[\"test\"])\n",
    "\n",
    "for example in ds[\"test\"]:\n",
    "    prompt = example[\"prompt\"]\n",
    "    completion = example[\"completion\"]\n",
    "    if check_translation(model, tokenizer, prompt, completion):\n",
    "        num_correct += 1\n",
    "\n",
    "print(f\"\\nResult: {num_correct}/{num_examples} correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-lora-hdr",
   "metadata": {},
   "source": [
    "The base model does OK, but it can do better.\n",
    "\n",
    "## Step 4. Configure LoRA and train the model\n",
    "\n",
    "We'll use Low-Rank Adaptation (LoRA) to make training fast and memory-efficient. LoRA adds a small number of new, trainable parameters to the model, freezing the original ones. This means we only have to update a tiny fraction of the model's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "demo-lora-config",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params: 3,686,400 / 138,201,408 (2.67%)\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=64,  # Rank of the update matrices. Lower is fewer parameters.\n",
    "    lora_alpha=16,  # LoRA scaling factor. Generally set to 16.\n",
    "    lora_dropout=0.05,  # Dropout for LoRA layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Wrap the base model with LoRA layers\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print the percentage of trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(\n",
    "    f\"Trainable params: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-training-args-hdr",
   "metadata": {},
   "source": [
    "Notice that we're only training about 2.67% of the total parameters! Now, we set the training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "demo-sftconfig",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Your setup doesn't support bf16/gpu. You need to assign use_cpu if you want to train the model on CPU",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Training arguments for the SFTTrainer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m training_args = \u001b[43mSFTConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata/model_demo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Directory to save artifacts\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Small batch size for demo\u001b[39;49;00m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Two forward and backward passes per update step\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Number of times to go through the data\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Controls how much the model weights are updated\u001b[39;49;00m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Log training progress every 10 steps\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mno\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Don't save model checkpoints\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Disable reporting to services like Weights & Biases\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use full precision (fp32) for wider compatibility\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:130\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, torch_empty_cache_steps, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, enable_jit_checkpoint, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, use_cpu, seed, data_seed, bf16, fp16, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_config, accelerator_config, parallelism_config, deepspeed, label_smoothing_factor, optim, optim_args, group_by_length, length_column_name, report_to, project, trackio_space_id, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, hub_revision, gradient_checkpointing, gradient_checkpointing_kwargs, include_for_metrics, eval_do_concat_batches, auto_find_batch_size, full_determinism, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, batch_eval_metrics, eval_on_start, use_liger_kernel, liger_kernel_config, eval_use_gather_object, average_tokens_across_devices, use_cache, model_init_kwargs, chat_template_path, dataset_text_field, dataset_kwargs, dataset_num_proc, eos_token, pad_token, max_length, shuffle_dataset, packing, packing_strategy, padding_free, pad_to_multiple_of, eval_packing, completion_only_loss, assistant_only_loss, loss_type, activation_offloading)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\victor.francheto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\trl\\trainer\\sft_config.py:279\u001b[39m, in \u001b[36mSFTConfig.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    277\u001b[39m     \u001b[38;5;28mself\u001b[39m.bf16 = \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.fp16) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__post_init__\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\victor.francheto\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1534\u001b[39m, in \u001b[36mTrainingArguments.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1532\u001b[39m                 error_message += \u001b[33m\"\u001b[39m\u001b[33m You need Ampere+ GPU with cuda>=11.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1533\u001b[39m             \u001b[38;5;66;03m# gpu\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1534\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_message)\n\u001b[32m   1536\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fp16 \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bf16:\n\u001b[32m   1537\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt most one of fp16 and bf16 can be True, but not both\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Your setup doesn't support bf16/gpu. You need to assign use_cpu if you want to train the model on CPU"
     ]
    }
   ],
   "source": [
    "# Training arguments for the SFTTrainer\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"data/model_demo\",  # Directory to save artifacts\n",
    "    per_device_train_batch_size=8,  # Small batch size for demo\n",
    "    gradient_accumulation_steps=2,  # Two forward and backward passes per update step\n",
    "    num_train_epochs=20,  # Number of times to go through the data\n",
    "    learning_rate=2e-4,  # Controls how much the model weights are updated\n",
    "    logging_steps=50,  # Log training progress every 10 steps\n",
    "    save_strategy=\"no\",  # Don't save model checkpoints\n",
    "    report_to=[],  # Disable reporting to services like Weights & Biases\n",
    "    fp16=False,  # Use full precision (fp32) for wider compatibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-trainer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0341d830b46a4f739e67a1623e124e7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/briancruz/udacity/udacity genai nd c1 project refresh 2025/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3888, 'grad_norm': 0.05742060765624046, 'learning_rate': 3.3333333333333335e-05, 'num_tokens': 21162.0, 'mean_token_accuracy': 0.9018999320268631, 'epoch': 14.29}\n",
      "{'train_runtime': 14.9863, 'train_samples_per_second': 65.393, 'train_steps_per_second': 4.004, 'train_loss': 0.32870734483003616, 'num_tokens': 25352.0, 'mean_token_accuracy': 1.0, 'epoch': 17.14}\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "# Create the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"test\"],\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "# Start the training process!\n",
    "print(\"--- Starting Training ---\")\n",
    "trainer.train()\n",
    "print(\"--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-eval-tuned-hdr",
   "metadata": {},
   "source": [
    "## Step 5. Evaluate the fine-tuned model\n",
    "\n",
    "Training is done! Now for the moment of truth. Let's see if our model learned the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-eval-tuned-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Fine-Tuned Model (After Training) ---\n",
      "Proposed: wryly-ish | Actual: wryly-ish | Correct: âœ…\n",
      "Proposed: 1-ish | Actual: glisten-ish | Correct: âŒ\n",
      "Proposed: quest-ish | Actual: quest-ish | Correct: âœ…\n",
      "Proposed: crave-ish | Actual: crave-ish | Correct: âœ…\n",
      "Proposed: lus-ish | Actual: lush-ish | Correct: âŒ\n",
      "Proposed: fable-ish | Actual: fable-ish | Correct: âœ…\n",
      "Proposed: knack-ish | Actual: knack-ish | Correct: âœ…\n",
      "Proposed: t-m-i-p-h-e-l-o | Actual: triumph-ish | Correct: âŒ\n",
      "Proposed: sapphire-ish | Actual: sapphire-ish | Correct: âœ…\n",
      "Proposed: expose-ish | Actual: expose-ish | Correct: âœ…\n",
      "Proposed: Frescos-ish | Actual: frescos-ish | Correct: âŒ\n",
      "Proposed: wisp-ish | Actual: wisp-ish | Correct: âœ…\n",
      "Proposed: mirage-ish | Actual: mirage-ish | Correct: âœ…\n",
      "\n",
      "Result: 9/13 correct.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Evaluating Fine-Tuned Model (After Training) ---\")\n",
    "num_correct = 0\n",
    "num_examples = len(ds[\"test\"])\n",
    "\n",
    "for example in ds[\"test\"]:\n",
    "    prompt = example[\"prompt\"]\n",
    "    completion = example[\"completion\"]\n",
    "    if check_translation(model, tokenizer, prompt, completion):\n",
    "        num_correct += 1\n",
    "\n",
    "print(f\"\\nResult: {num_correct}/{num_examples} correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demo-conclusion",
   "metadata": {},
   "source": [
    "## Conclusion ðŸŽ‰\n",
    "\n",
    "Success! After a very short training run on a tiny dataset, the model improved on the task. It went from 7/13 to 9/13, which is modest, but shows fine-tuning with parameter-efficient fine-tuning works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a13f83b",
   "metadata": {},
   "source": [
    "<br /><br /><br /><br /><br /><br /><br /><br />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
