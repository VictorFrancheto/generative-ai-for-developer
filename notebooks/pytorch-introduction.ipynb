{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkDNtWF0zElF"
      },
      "source": [
        "# Introduction to PyTorch\n",
        "\n",
        "PyTorch is a key tool for machine learning and deep learning. Think of it as a mechanic's toolbox, providing everything you need to easily build and train complex models. This notebook will give you a solid understanding of PyTorch's core concepts, enabling you to expand your knowledge and explore the depths of machine learning models.\n",
        "\n",
        "## Outline\n",
        "\n",
        "We'll cover the following topics:\n",
        "* **PyTorch Tensors**: Multidimensional arrays and the foundational data structure in PyTorch.\n",
        "* **Neural Nets in PyTorch**: Handled by the useful `torch.nn` module.\n",
        "* **PyTorch Loss Functions**: Define the objective function we wish to minimize during training.\n",
        "* **PyTorch Optimizers**: The algorithms we use for minimizing our loss functions.\n",
        "* **Datasets and Data Loaders in PyTorch**: Useful tools for working with data at scale easily.\n",
        "* **Training loops in PyTorch**: Define the fundamental steps for training a deep learning model.\n",
        "\n",
        "## PyTorch Tensors\n",
        "\n",
        "Drawing a parallel to mathematics, tensors can be likened to vectors and matrices, but with the capability to have more than two dimensions.\n",
        "\n",
        "Let’s take a look at using pytorch tensors to store images. Suppose we have four grayscale images, each of size 28x28 pixels. We can represent these images as a 4x28x28 tensor in PyTorch. Here, `images` is a 3D tensor where the first dimension corresponds to the number of images, and the subsequent dimensions represent the height and width of each image. In this case we’ll create 4 images of random noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wgwK59Ny6s7",
        "outputId": "df5de033-b26d-4cea-9dbd-3c0cf40e7c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.8366e-02, 5.4725e-01, 8.4380e-01, 3.5491e-01, 7.8561e-01, 4.6344e-01,\n",
            "         3.2411e-01, 8.7513e-01, 8.3474e-01, 1.5680e-01, 7.4894e-01, 4.9487e-01,\n",
            "         8.2507e-01, 8.4637e-01, 4.9468e-02, 8.3969e-01, 6.1561e-01, 2.8184e-01,\n",
            "         3.9038e-01, 8.0175e-01, 8.8512e-01, 9.3458e-01, 2.9582e-01, 2.5265e-01,\n",
            "         8.1913e-01, 1.7062e-01, 3.4448e-01, 4.4747e-01],\n",
            "        [6.4026e-01, 1.1694e-01, 3.6104e-01, 5.3472e-01, 1.9283e-01, 7.1908e-01,\n",
            "         9.1120e-01, 8.3796e-01, 2.3296e-01, 6.2427e-01, 9.1250e-01, 5.8930e-01,\n",
            "         9.2085e-01, 8.3160e-01, 2.9857e-01, 6.0772e-02, 9.6466e-02, 3.7611e-02,\n",
            "         4.2116e-01, 9.4699e-01, 1.8819e-01, 5.6174e-01, 9.0469e-01, 3.0889e-01,\n",
            "         1.3949e-01, 5.5286e-01, 8.1606e-01, 3.6672e-01],\n",
            "        [6.4918e-01, 8.2750e-01, 1.3733e-01, 3.6943e-01, 9.4366e-01, 8.1554e-01,\n",
            "         9.2853e-01, 1.5553e-01, 5.8044e-01, 3.9109e-01, 4.0178e-01, 1.3676e-01,\n",
            "         9.0209e-01, 9.4271e-01, 4.2274e-01, 9.5165e-02, 5.5381e-01, 8.4054e-01,\n",
            "         7.1969e-01, 7.7532e-01, 1.3733e-01, 6.2561e-01, 4.8684e-01, 7.8439e-01,\n",
            "         6.6568e-01, 1.4108e-04, 5.9760e-01, 2.7293e-01],\n",
            "        [4.6496e-01, 1.3245e-01, 1.0038e-01, 6.1052e-01, 4.2307e-01, 6.0506e-01,\n",
            "         9.9439e-01, 2.3115e-02, 9.2263e-01, 2.2232e-01, 4.9173e-02, 7.1328e-01,\n",
            "         4.3393e-01, 9.8640e-01, 4.5563e-01, 3.7943e-01, 8.2701e-01, 2.2337e-01,\n",
            "         5.5896e-01, 9.5182e-01, 4.8419e-01, 7.0264e-01, 3.4266e-01, 9.6332e-01,\n",
            "         2.8294e-01, 6.2960e-01, 1.7782e-01, 9.0527e-01],\n",
            "        [5.0487e-01, 4.5661e-01, 1.0885e-01, 5.6069e-01, 6.5149e-01, 7.1099e-01,\n",
            "         9.7593e-01, 1.3922e-01, 6.2526e-01, 6.1705e-01, 4.2659e-01, 5.8921e-01,\n",
            "         5.6909e-01, 9.4321e-01, 3.8693e-01, 1.3237e-01, 6.0359e-01, 3.0392e-01,\n",
            "         6.0866e-01, 7.2702e-01, 8.4507e-01, 8.5729e-01, 8.7845e-03, 6.7875e-01,\n",
            "         2.5944e-01, 6.2448e-01, 5.7256e-01, 7.0089e-01],\n",
            "        [7.9586e-01, 5.7531e-01, 1.6308e-01, 7.6668e-01, 5.7850e-01, 3.6375e-01,\n",
            "         7.0042e-01, 5.7391e-01, 8.4429e-01, 6.6137e-01, 2.7732e-01, 9.8291e-01,\n",
            "         3.0008e-01, 8.7459e-01, 2.1637e-01, 7.1697e-01, 5.4987e-02, 8.8406e-01,\n",
            "         8.5151e-01, 4.8955e-01, 4.1585e-01, 6.4520e-01, 2.0724e-01, 8.6484e-01,\n",
            "         3.0342e-01, 8.1366e-01, 3.0410e-01, 4.7623e-01],\n",
            "        [7.9750e-01, 9.2285e-01, 9.8723e-01, 6.6202e-01, 8.1981e-01, 1.3473e-01,\n",
            "         7.2775e-01, 3.2428e-01, 8.9597e-01, 1.4337e-01, 4.4425e-01, 7.9886e-01,\n",
            "         7.0790e-01, 2.5587e-01, 4.9436e-02, 3.2297e-01, 6.8535e-01, 5.0954e-01,\n",
            "         3.6660e-01, 5.7618e-01, 8.1097e-01, 6.8185e-01, 8.1637e-01, 3.0345e-01,\n",
            "         1.0154e-01, 2.7576e-01, 2.2638e-01, 8.8687e-01],\n",
            "        [3.6200e-01, 9.7932e-01, 2.9934e-01, 6.4467e-01, 5.3777e-01, 4.9879e-01,\n",
            "         4.0176e-02, 3.1059e-01, 9.8783e-01, 7.1927e-02, 9.5108e-01, 4.3709e-01,\n",
            "         9.1124e-01, 3.0256e-01, 1.5103e-01, 6.4569e-01, 9.2048e-01, 4.9772e-01,\n",
            "         2.6412e-02, 5.5943e-01, 8.9792e-01, 5.6573e-01, 7.0122e-01, 8.2969e-01,\n",
            "         6.8790e-03, 7.9336e-01, 4.4188e-01, 4.8888e-02],\n",
            "        [8.1839e-01, 2.7145e-01, 9.2015e-01, 2.3480e-01, 5.6870e-01, 6.6819e-01,\n",
            "         8.5263e-01, 8.9628e-01, 3.9788e-01, 2.6582e-01, 3.8970e-01, 4.1065e-01,\n",
            "         7.3428e-01, 3.2056e-01, 3.3760e-01, 7.4445e-01, 5.2930e-01, 9.0162e-01,\n",
            "         5.2349e-01, 7.7581e-02, 1.2590e-01, 5.2020e-01, 4.7176e-01, 4.8491e-01,\n",
            "         4.7080e-01, 8.7460e-01, 7.0453e-01, 8.3139e-01],\n",
            "        [7.4614e-01, 1.1275e-01, 8.8869e-01, 9.1549e-01, 6.7125e-01, 9.2167e-01,\n",
            "         7.9172e-01, 6.7372e-01, 6.7440e-01, 4.9470e-01, 1.4267e-01, 1.6367e-01,\n",
            "         5.6644e-01, 7.3528e-01, 8.6149e-01, 3.1937e-01, 9.0079e-02, 1.0477e-01,\n",
            "         4.6624e-01, 7.5568e-01, 2.9365e-01, 1.6927e-01, 5.0624e-01, 3.5966e-01,\n",
            "         6.0955e-01, 4.9479e-01, 2.9522e-01, 5.5366e-01],\n",
            "        [4.8882e-04, 3.9186e-01, 2.4333e-01, 5.7743e-02, 3.4917e-01, 4.9496e-01,\n",
            "         4.1179e-01, 5.4848e-01, 8.3935e-01, 8.4674e-03, 9.1404e-01, 3.0853e-02,\n",
            "         3.9004e-01, 2.0207e-01, 6.3773e-02, 9.2866e-01, 6.3381e-01, 8.3988e-02,\n",
            "         9.6871e-01, 4.6984e-01, 2.0329e-01, 3.0353e-02, 3.7168e-01, 9.7362e-01,\n",
            "         5.3625e-01, 8.2755e-01, 7.0195e-03, 1.9540e-01],\n",
            "        [8.6338e-01, 8.8879e-01, 2.1570e-01, 4.7936e-01, 6.1322e-01, 1.9038e-01,\n",
            "         8.4363e-01, 4.0503e-01, 8.6392e-01, 8.8355e-01, 2.5893e-01, 7.6251e-01,\n",
            "         1.9415e-01, 2.3531e-01, 2.0257e-01, 4.4961e-01, 6.4120e-01, 1.8064e-01,\n",
            "         4.7587e-01, 7.3695e-01, 2.5971e-01, 1.5789e-01, 4.1289e-01, 4.6963e-01,\n",
            "         5.5240e-01, 3.3888e-01, 1.5528e-01, 1.5473e-01],\n",
            "        [4.4063e-01, 1.4891e-01, 2.5391e-01, 1.9057e-01, 4.3941e-01, 2.6707e-01,\n",
            "         3.6353e-01, 1.3724e-01, 2.0196e-01, 4.8426e-01, 9.0285e-01, 7.3268e-01,\n",
            "         2.1142e-01, 6.2753e-01, 4.9892e-01, 3.5054e-01, 1.0287e-01, 9.7130e-01,\n",
            "         9.8708e-01, 2.2944e-01, 9.9419e-01, 5.9278e-01, 4.1627e-01, 1.0305e-02,\n",
            "         6.4426e-02, 7.6250e-01, 9.9698e-01, 9.0784e-01],\n",
            "        [1.6918e-01, 5.2819e-01, 3.6192e-01, 5.2015e-02, 9.6128e-01, 6.7023e-01,\n",
            "         1.9213e-01, 6.0041e-01, 3.4474e-01, 6.4451e-01, 2.5124e-01, 1.9184e-02,\n",
            "         4.7944e-02, 6.4870e-02, 3.2288e-01, 3.1029e-01, 2.8269e-01, 8.5892e-01,\n",
            "         7.9435e-01, 3.5311e-01, 1.3066e-01, 9.0240e-01, 3.5203e-01, 2.7432e-02,\n",
            "         1.7607e-01, 8.8442e-01, 5.4341e-01, 3.9154e-01],\n",
            "        [9.3480e-01, 3.0274e-01, 9.1657e-01, 5.6708e-01, 8.0194e-01, 4.6274e-02,\n",
            "         4.9735e-01, 2.4868e-01, 5.5883e-01, 3.0940e-01, 1.1455e-01, 9.8850e-01,\n",
            "         4.7291e-01, 8.3704e-01, 6.3502e-01, 3.8128e-01, 4.2209e-01, 6.7335e-01,\n",
            "         4.7167e-01, 2.1500e-01, 7.4222e-01, 2.6591e-01, 5.9322e-01, 7.7931e-01,\n",
            "         2.1358e-01, 5.6589e-01, 9.5505e-01, 9.9983e-01],\n",
            "        [6.0379e-01, 2.1957e-01, 4.5421e-01, 2.7982e-01, 6.1264e-01, 8.6798e-01,\n",
            "         2.0742e-01, 3.6043e-01, 9.1812e-01, 6.3628e-01, 4.0501e-01, 8.2337e-01,\n",
            "         1.5033e-01, 7.7542e-01, 3.3369e-01, 5.5522e-01, 4.2303e-01, 1.6040e-01,\n",
            "         7.9030e-01, 4.4763e-01, 8.2544e-01, 5.7304e-01, 8.2240e-01, 3.8645e-01,\n",
            "         1.4350e-01, 7.0185e-01, 4.1912e-01, 8.8111e-01],\n",
            "        [8.7050e-01, 9.8211e-01, 1.7847e-01, 4.2344e-01, 5.0445e-01, 8.0980e-01,\n",
            "         5.2808e-01, 6.9485e-01, 9.0708e-01, 7.8130e-01, 6.3126e-01, 7.8410e-01,\n",
            "         5.0204e-01, 7.4053e-01, 8.3532e-02, 3.5088e-01, 2.9847e-01, 9.2433e-01,\n",
            "         6.9544e-01, 9.3576e-01, 7.8366e-01, 3.6720e-02, 8.8134e-01, 8.7138e-01,\n",
            "         1.0345e-01, 4.5982e-01, 5.3820e-01, 7.2390e-01],\n",
            "        [7.0535e-01, 6.6195e-01, 6.2585e-01, 4.0795e-01, 6.9669e-01, 2.5585e-01,\n",
            "         7.2927e-01, 4.4470e-01, 7.9461e-01, 2.4738e-01, 3.0464e-01, 4.4612e-01,\n",
            "         7.7343e-01, 6.4714e-01, 4.1497e-01, 2.2218e-01, 2.1672e-01, 2.3336e-01,\n",
            "         4.7233e-01, 6.5809e-01, 1.2317e-01, 5.3870e-01, 6.7274e-01, 2.9383e-02,\n",
            "         7.1959e-01, 4.1345e-01, 5.7753e-02, 7.7134e-02],\n",
            "        [3.8156e-01, 7.5045e-02, 7.0904e-01, 5.5918e-01, 6.1278e-01, 1.5331e-01,\n",
            "         2.3766e-01, 8.7084e-02, 4.7974e-02, 1.2106e-01, 9.0406e-02, 5.1860e-01,\n",
            "         5.0215e-01, 6.1699e-01, 1.0492e-01, 6.8881e-01, 5.4236e-02, 9.7035e-01,\n",
            "         8.3742e-01, 4.6741e-01, 1.7634e-02, 5.6853e-01, 5.3612e-01, 9.9299e-01,\n",
            "         5.7417e-01, 7.8117e-01, 8.7993e-01, 9.3593e-01],\n",
            "        [1.4381e-01, 1.6851e-01, 7.0488e-01, 5.9303e-01, 9.1099e-01, 6.8598e-01,\n",
            "         9.8563e-01, 9.9389e-01, 7.0594e-01, 5.8904e-02, 1.7113e-01, 8.9782e-01,\n",
            "         8.1053e-01, 9.2751e-01, 6.0377e-01, 2.7104e-01, 2.8253e-01, 7.6963e-01,\n",
            "         3.7271e-01, 6.6304e-01, 5.9363e-01, 9.6093e-01, 6.1394e-01, 3.0265e-01,\n",
            "         3.5497e-01, 9.0797e-02, 9.2901e-01, 7.9492e-01],\n",
            "        [7.6336e-01, 7.7449e-01, 6.2063e-01, 3.7328e-02, 4.0091e-01, 3.0967e-01,\n",
            "         1.6351e-01, 6.3925e-01, 9.6294e-01, 4.5315e-01, 1.0279e-01, 3.3911e-01,\n",
            "         3.1544e-01, 2.3322e-01, 6.4063e-01, 6.4488e-01, 2.9280e-01, 7.7530e-01,\n",
            "         9.8867e-01, 5.9087e-01, 3.8565e-01, 2.2062e-01, 4.9815e-01, 6.1189e-01,\n",
            "         2.7740e-01, 8.4183e-01, 7.6625e-01, 5.4827e-01],\n",
            "        [3.5774e-01, 2.9226e-01, 4.2446e-01, 3.5917e-01, 2.0123e-01, 1.7104e-01,\n",
            "         7.5987e-01, 4.1841e-02, 6.0082e-01, 7.7472e-01, 8.2248e-01, 1.3288e-01,\n",
            "         6.7049e-01, 1.5631e-01, 7.8118e-01, 2.5936e-01, 1.5548e-01, 4.1018e-01,\n",
            "         8.6700e-01, 6.8760e-01, 6.4357e-01, 1.9287e-01, 8.4760e-02, 5.0933e-01,\n",
            "         9.0322e-01, 7.6591e-01, 4.7233e-02, 9.4564e-01],\n",
            "        [2.4540e-01, 9.2985e-01, 6.1537e-01, 6.9651e-01, 1.9674e-01, 3.9584e-01,\n",
            "         8.4049e-01, 7.7859e-01, 4.1469e-01, 8.3387e-01, 5.0616e-02, 9.7035e-01,\n",
            "         2.9556e-01, 3.9943e-01, 8.9009e-02, 2.7136e-01, 5.0656e-01, 4.3882e-01,\n",
            "         5.2127e-01, 7.9184e-01, 5.8270e-01, 6.0446e-01, 6.6229e-01, 4.5101e-01,\n",
            "         8.8669e-02, 7.6610e-02, 9.7973e-01, 1.2006e-01],\n",
            "        [8.4823e-01, 9.8324e-01, 4.2740e-01, 6.8442e-01, 1.1045e-01, 9.2332e-01,\n",
            "         6.2064e-02, 5.3455e-01, 9.6473e-01, 2.5472e-01, 7.0985e-01, 6.5490e-01,\n",
            "         2.2675e-02, 5.3645e-03, 3.4181e-01, 6.6937e-01, 9.2917e-01, 8.5332e-01,\n",
            "         9.3375e-01, 8.0566e-01, 8.7429e-01, 9.2014e-01, 1.4952e-01, 6.6659e-01,\n",
            "         3.4582e-01, 1.4368e-01, 6.2706e-01, 1.5227e-01],\n",
            "        [5.4537e-01, 8.8751e-02, 4.8812e-01, 3.4522e-01, 6.6139e-01, 3.9872e-01,\n",
            "         2.9518e-01, 1.6703e-01, 3.7553e-01, 7.4422e-01, 6.5757e-01, 3.5362e-01,\n",
            "         9.7559e-01, 8.6196e-03, 3.4901e-02, 1.0628e-01, 6.0958e-01, 8.3968e-02,\n",
            "         2.2513e-01, 2.2209e-01, 6.2019e-01, 3.4834e-01, 6.5351e-01, 8.9027e-01,\n",
            "         2.5877e-01, 5.6222e-01, 8.9336e-01, 9.5083e-01],\n",
            "        [2.2978e-01, 6.5208e-01, 3.6358e-01, 3.9141e-01, 9.2136e-01, 9.2832e-02,\n",
            "         1.0217e-01, 3.5045e-01, 3.2642e-01, 5.4816e-01, 4.2104e-01, 9.5507e-01,\n",
            "         4.1866e-01, 9.3730e-01, 9.2541e-01, 4.0722e-01, 3.6997e-01, 5.6123e-01,\n",
            "         4.1654e-01, 1.7317e-02, 1.6191e-01, 5.2208e-01, 1.6844e-01, 8.8441e-01,\n",
            "         3.8136e-01, 6.9457e-01, 1.2372e-02, 8.4537e-01],\n",
            "        [2.0972e-01, 7.3047e-01, 2.2702e-01, 6.0838e-01, 6.0048e-01, 2.1617e-02,\n",
            "         6.8415e-01, 7.7884e-01, 3.2326e-01, 3.2222e-01, 2.4645e-01, 9.9293e-02,\n",
            "         4.4178e-01, 7.1661e-01, 4.7017e-01, 5.9091e-01, 9.9425e-02, 8.0448e-01,\n",
            "         2.5767e-01, 4.5620e-01, 9.4291e-01, 4.5198e-01, 6.6761e-01, 5.7774e-01,\n",
            "         9.8763e-01, 1.5810e-01, 7.4516e-01, 1.5922e-01],\n",
            "        [3.1384e-01, 2.4368e-01, 3.1459e-01, 6.7813e-01, 6.5110e-01, 4.1365e-01,\n",
            "         3.5637e-02, 4.4449e-01, 1.8185e-01, 3.7310e-01, 4.8321e-01, 9.1528e-01,\n",
            "         1.0314e-01, 1.8069e-01, 1.1165e-01, 4.6665e-01, 3.2239e-01, 9.1690e-01,\n",
            "         7.0323e-01, 6.6977e-01, 2.8103e-01, 3.5595e-01, 5.7110e-01, 9.9781e-01,\n",
            "         8.7865e-01, 3.3613e-01, 3.2331e-01, 7.8679e-01]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "images = torch.rand(4, 28, 28)\n",
        "# To access the second image\n",
        "second_image = images[1]\n",
        "print(second_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5y93CppzRcP"
      },
      "source": [
        "To display an image from a PyTorch tensor, you can use the `matplotlib` library. Here's an example of how you would display the image. We use the `imshow` function from `matplotlib.pyplot` to display the image. The `cmap='gray'` argument ensures that the image is displayed in grayscale."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "-csOWKzYzNTO",
        "outputId": "0eb59e4e-1b7d-4193-d012-ef98ebd590f7"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.imshow(second_image, cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_ZkalRDzVkD"
      },
      "source": [
        "Since we used `torch.rand` to generate these four images randomly, we get noise as expected.\n",
        "If you are familiar with matrices, then you will be at home with PyTorch tensors, since they offer all the same things you can do with a matrix. For instance, take the 2x2 matrix A. Let’s calculate the 1st, 2nd, 3rd, and 4th powers of this matrix, using matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jJdDpDwzUBt",
        "outputId": "dfd9ea1f-0e70-45b4-8185-ee0a969aa33a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A^1:\n",
            " tensor([[1., 1.],\n",
            "        [1., 0.]])\n",
            "A^2:\n",
            " tensor([[2., 1.],\n",
            "        [1., 1.]])\n",
            "A^3:\n",
            " tensor([[3., 2.],\n",
            "        [2., 1.]])\n",
            "A^4:\n",
            " tensor([[5., 3.],\n",
            "        [3., 2.]])\n"
          ]
        }
      ],
      "source": [
        "A = torch.tensor([[1, 1], [1, 0]], dtype=torch.float32)\n",
        "print(\"A^1:\\n\", torch.matrix_power(A, 1))\n",
        "print(\"A^2:\\n\", torch.matrix_power(A, 2))\n",
        "print(\"A^3:\\n\", torch.matrix_power(A, 3))\n",
        "print(\"A^4:\\n\", torch.matrix_power(A, 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRdZ4vlfzX14"
      },
      "source": [
        "## Neural Nets in PyTorch\n",
        "\n",
        "Moving on, PyTorch provides robust tools for the creation and manipulation of neural networks. Think of neural networks as intricate systems of interconnected nodes. Each node or neuron processes input and transmits the result to subsequent nodes. The simplest form of a neural network is a Multi-Layer Perceptron (MLP). In an MLP, there is an input layer, an output layer, and any number of hidden layers in the middle. Let’s make a Multi-Layer Perceptron with one hidden layer using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OJyT5tR2zXM7"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.hidden_layer = nn.Linear(input_size, 64)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(64, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden_layer(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_enRosezb6T"
      },
      "source": [
        "We can now instantiate the multi-layer perceptron with an input size of 10 and print the model to review the sizes of our layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKT9hchYzaaa",
        "outputId": "2e8a048e-27fa-4abb-c9e6-f0253ff977a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP(\n",
            "  (hidden_layer): Linear(in_features=10, out_features=64, bias=True)\n",
            "  (activation): ReLU()\n",
            "  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = MLP(input_size=10)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpLLGJGTzfV1"
      },
      "source": [
        "We can test out the `forward` method of the model by passing in a vector of length 10. As expected, we receive a vector of length 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ITy4GNtzddR",
        "outputId": "9fc4f66c-c248-4ed2-ad93-8a6dd0b866fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.0060,  0.1224], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "input_vector = torch.randn(10)\n",
        "output = model(input_vector)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeyfnszazhHX"
      },
      "source": [
        "## Loss Functions\n",
        "\n",
        "Let’s discuss loss functions in PyTorch now. Loss functions play a pivotal role in guiding model optimization. These functions quantify the discrepancy between the predicted output and the actual target values. We’ll look at two very important building blocks: Cross-Entropy Loss and Mean Squared Error (MSE) Loss.\n",
        "\n",
        "We start with a cross-entropy loss function, which is suited for classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHuuq-S_zgwn",
        "outputId": "b1bb4ace-36ef-4e15-c06b-5e75a82e500c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss when prediction is more likely a dog: 0.0486\n",
            "Loss when prediction is more likely a cat: 0.9130\n"
          ]
        }
      ],
      "source": [
        "cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "target = torch.tensor([1]) # 0 for cats, 1 for dogs. Label is for dog.\n",
        "\n",
        "# Case 1: Prediction is more likely a dog\n",
        "predicted1 = torch.tensor([[2.0, 5.0]])\n",
        "loss1 = cross_entropy_loss(predicted1, target)\n",
        "print(f\"Loss when prediction is more likely a dog: {loss1.item():.4f}\")\n",
        "\n",
        "# Case 2: Prediction is more likely a cat\n",
        "predicted2 = torch.tensor([[1.5, 1.1]])\n",
        "loss2 = cross_entropy_loss(predicted2, target)\n",
        "print(f\"Loss when prediction is more likely a cat: {loss2.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KACFBI6zl0r"
      },
      "source": [
        "Mean Squared Error (MSE) Loss is commonly employed in regression problems. Consider a situation where we aim to predict house prices. Let's say for a specific house, our model predicts a price of $320,000, but the actual price is $300,000."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLv9mmNXzj0l",
        "outputId": "9dbaa75d-15c0-4def-8c06-f2e9494c834b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MSE Loss: 400000000.0\n"
          ]
        }
      ],
      "source": [
        "mse_loss = nn.MSELoss()\n",
        "predicted_price = torch.tensor([320000.0])\n",
        "actual_price = torch.tensor([300000.0])\n",
        "loss = mse_loss(predicted_price, actual_price)\n",
        "print(f\"MSE Loss: {loss.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D05mNXzzoQq"
      },
      "source": [
        "## PyTorch Optimizers\n",
        "\n",
        "After we have a loss function, we need to train the model by minimizing its loss function. For this, we use a PyTorch optimizer. One popular optimizer is Stochastic Gradient Descent (SGD)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZeBOZb0znuh",
        "outputId": "109c3ce5-dbcc-4b29-9a37-ae07d9876661"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SGD Optimizer instantiated.\n",
            "Adam Optimizer instantiated.\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import SGD, Adam\n",
        "\n",
        "# Assuming 'model' is our MLP defined earlier\n",
        "optimizer_sgd = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "print(\"SGD Optimizer instantiated.\")\n",
        "\n",
        "# Adam is another popular optimizer.\n",
        "optimizer_adam = Adam(model.parameters(), lr=0.01)\n",
        "print(\"Adam Optimizer instantiated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eBSNwLizt7r"
      },
      "source": [
        "## Datasets and DataLoaders\n",
        "\n",
        "In the PyTorch framework, the handling of data is facilitated through the `Dataset` class and the `DataLoader` utility. The `Dataset` class serves as a blueprint for how data is accessed and transformed, while the `DataLoader` is a utility that wraps around a `Dataset` object to provide batched, shuffled, and parallelized loading of data.\n",
        "\n",
        "Let's design an example to define a dataset for returning pairs of numbers and their products."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDsN8GoHzsAN",
        "outputId": "816a490b-d787-4672-99ca-d0f5c89a8ba1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: 3, 4, Output: 12\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class NumberProductDataset(Dataset):\n",
        "    def __init__(self, data_range):\n",
        "        self.data = [(i, i + 1, i * (i + 1)) for i in range(data_range)]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "dataset = NumberProductDataset(data_range=11)\n",
        "sample = dataset[3] # pick the 4th element (index 3)\n",
        "print(f\"Input: {sample[0]}, {sample[1]}, Output: {sample[2]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLqOigm-z0JV"
      },
      "source": [
        "Now let's turn to the example of the `DataLoader`. Our data loader will take our `NumberProductDataset` as an input. We will also specify a batch size of three and set shuffle to true."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxavcwwkzv5H",
        "outputId": "b358b851-fdf3-4127-83a2-e21a199ddf46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch 1:\n",
            "  Inputs 1: tensor([ 0,  8, 10])\n",
            "  Inputs 2: tensor([ 1,  9, 11])\n",
            "  Outputs: tensor([  0,  72, 110])\n",
            "Batch 2:\n",
            "  Inputs 1: tensor([1, 3, 4])\n",
            "  Inputs 2: tensor([2, 4, 5])\n",
            "  Outputs: tensor([ 2, 12, 20])\n",
            "Batch 3:\n",
            "  Inputs 1: tensor([9, 5, 7])\n",
            "  Inputs 2: tensor([10,  6,  8])\n",
            "  Outputs: tensor([90, 30, 56])\n",
            "Batch 4:\n",
            "  Inputs 1: tensor([6, 2])\n",
            "  Inputs 2: tensor([7, 3])\n",
            "  Outputs: tensor([42,  6])\n"
          ]
        }
      ],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)\n",
        "for i, (input1, input2, output) in enumerate(dataloader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"  Inputs 1: {input1}\")\n",
        "    print(f\"  Inputs 2: {input2}\")\n",
        "    print(f\"  Outputs: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKNkBd1_z2Ry"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "Ok. Now we’re at the final step. Let's bring together all the components of PyTorch and show a full-fledged training loop!\n",
        "\n",
        "For this example, we will create a simple dataset, consisting of pairs of numbers and their sums."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIbOh6ooz1nF",
        "outputId": "dfc479bd-df99-4cba-ee58-ecf93e14d1aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([0., 0.]), tensor(0.))\n",
            "(tensor([0., 1.]), tensor(1.))\n",
            "(tensor([0., 2.]), tensor(2.))\n",
            "(tensor([0., 3.]), tensor(3.))\n",
            "(tensor([0., 4.]), tensor(4.))\n"
          ]
        }
      ],
      "source": [
        "class NumberSumDataset(Dataset):\n",
        "    def __init__(self, data_range):\n",
        "        self.data = []\n",
        "        for i in range(data_range):\n",
        "            for j in range(data_range):\n",
        "                self.data.append(([i, j], i + j))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][0], dtype=torch.float32), torch.tensor(self.data[idx][1], dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# Print the first five elements to verify\n",
        "sum_dataset = NumberSumDataset(data_range=10)\n",
        "for i in range(5):\n",
        "    print(sum_dataset[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_uTXduOz6ce"
      },
      "source": [
        "Next, we define a multi-layer perceptron with 128 nodes in its hidden layer and a single node in its output layer. For the activation function, we use a rectified linear unit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YpM_0isGz4xY"
      },
      "outputs": [],
      "source": [
        "class SumMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SumMLP, self).__init__()\n",
        "        self.hidden_layer = nn.Linear(2, 128)\n",
        "        self.activation = nn.ReLU()\n",
        "        self.output_layer = nn.Linear(128, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden_layer(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYIaEYgdz9fy"
      },
      "source": [
        "All of the components needed for the training loop are listed here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4vWDD29iz79P"
      },
      "outputs": [],
      "source": [
        "sum_dataset = NumberSumDataset(data_range=10)\n",
        "sum_dataloader = DataLoader(sum_dataset, batch_size=10, shuffle=True)\n",
        "sum_model = SumMLP()\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = Adam(sum_model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5EyVi6h0AkZ"
      },
      "source": [
        "Here is the training loop!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06iQ4puWz_Yb",
        "outputId": "a9c26fd0-aa3c-4baa-fabb-7c7d0ebd2896"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Loss: 14.9183\n",
            "Epoch 2/20, Loss: 2.6584\n",
            "Epoch 3/20, Loss: 1.5372\n",
            "Epoch 4/20, Loss: 0.4976\n",
            "Epoch 5/20, Loss: 0.1549\n",
            "Epoch 6/20, Loss: 0.0614\n",
            "Epoch 7/20, Loss: 0.0358\n",
            "Epoch 8/20, Loss: 0.0172\n",
            "Epoch 9/20, Loss: 0.0090\n",
            "Epoch 10/20, Loss: 0.0060\n",
            "Epoch 11/20, Loss: 0.0042\n",
            "Epoch 12/20, Loss: 0.0038\n",
            "Epoch 13/20, Loss: 0.0029\n",
            "Epoch 14/20, Loss: 0.0028\n",
            "Epoch 15/20, Loss: 0.0031\n",
            "Epoch 16/20, Loss: 0.0025\n",
            "Epoch 17/20, Loss: 0.0020\n",
            "Epoch 18/20, Loss: 0.0012\n",
            "Epoch 19/20, Loss: 0.0011\n",
            "Epoch 20/20, Loss: 0.0010\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for inputs, targets in sum_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = sum_model(inputs)\n",
        "        loss = loss_function(predictions.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(sum_dataloader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCb0cyHl0E-s"
      },
      "source": [
        "Now it's time to try our model out! What about using the numbers 3 and 7?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOHkAqsE0DSk",
        "outputId": "0a3abf54-9d22-466e-eeb1-d583d00a1d79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction for 3 + 7: 10.0\n"
          ]
        }
      ],
      "source": [
        "test_input = torch.tensor([3.0, 7.0])\n",
        "prediction = sum_model(test_input)\n",
        "print(f\"Prediction for 3 + 7: {prediction.item():.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFkCp39X0G_k"
      },
      "source": [
        "Now you understand the core building blocks of PyTorch, equipping you with essential tools for developing powerful machine learning models."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
