{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e9bc598",
   "metadata": {},
   "source": [
    "# üéØ Evaluation Techniques for Generative AI Models\n",
    "\n",
    "Welcome to this comprehensive guide on evaluating Generative AI model outputs! This notebook demonstrates several essential techniques used to measure the quality and accuracy of AI-generated content.\n",
    "\n",
    "## üìö What You'll Learn\n",
    "- **Exact Match**: Strict comparison for precise answers\n",
    "- **ROUGE**: Lexical similarity for text evaluation\n",
    "- **Semantic Similarity**: Meaning-based comparison using embeddings\n",
    "- **Functional Correctness**: Code validation through unit tests\n",
    "- **Pass@k**: Multiple attempt success rate\n",
    "- **LLM-as-a-Judge**: AI-powered subjective evaluation\n",
    "\n",
    "## üöÄ Getting Started\n",
    "\n",
    "> **üìù Note:** First, we'll import the necessary libraries including tools for data handling, mathematical operations, and specialized evaluation metrics. Setting a random seed ensures reproducible results across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65301ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import random\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324bca82",
   "metadata": {},
   "source": [
    "## üéØ Exact Match (EM)\n",
    "\n",
    "### Overview\n",
    "Exact Match is the **simplest and strictest** evaluation metric available. It verifies whether the model's output is **perfectly identical** to the reference answer after normalization.\n",
    "\n",
    "### When to Use\n",
    "- ‚úÖ Multiple-choice questions\n",
    "- ‚úÖ Tasks with single, clear correct answers\n",
    "- ‚úÖ Classification tasks with specific labels\n",
    "- ‚ùå Open-ended text generation\n",
    "- ‚ùå Creative writing tasks\n",
    "\n",
    "### How It Works\n",
    "The metric normalizes both strings (lowercase, trim whitespace) and returns 1 for a perfect match, 0 otherwise.\n",
    "\n",
    "> **üìù Note:** In this example, we compare predicted fruit names against correct labels. We define a simple normalize function to make text lowercase and remove extra whitespace before comparing. The final score is the average of all individual comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6281321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare predicted fruit names with the correct labels.\n",
    "preds = [\"Apple\", \"banana \", \" Orange\"]\n",
    "labels = [\"apple\", \"banana\", \"grape\"]\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    \"\"\"Normalize a string by lowercasing and stripping whitespace.\"\"\"\n",
    "    return s.lower().strip()\n",
    "\n",
    "def exact_match(pred: str, label: str) -> int:\n",
    "    \"\"\"Return 1 if normalized strings are identical, else 0.\"\"\"\n",
    "    return int(normalize(pred) == normalize(label))\n",
    "\n",
    "# Calculate EM score for each pair\n",
    "em_scores = [exact_match(p, l) for p, l in zip(preds, labels)]\n",
    "\n",
    "# The final score is the average of individual scores\n",
    "em_accuracy = sum(em_scores) / len(em_scores)\n",
    "\n",
    "print(f\"Individual Scores: {em_scores}\")\n",
    "print(f\"Average Exact Match Accuracy: {em_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f27642",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "As shown in the output, **two out of three** predictions match perfectly after normalization, resulting in an average accuracy of approximately **67%**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Lexical Similarity (ROUGE)\n",
    "\n",
    "### Overview\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a more flexible metric that measures the **overlap of words or n-grams** between the model's prediction and the reference text.\n",
    "\n",
    "### When to Use\n",
    "- ‚úÖ Text summarization\n",
    "- ‚úÖ Machine translation\n",
    "- ‚úÖ Tasks where answers can be phrased differently\n",
    "- ‚úÖ Content paraphrasing evaluation\n",
    "\n",
    "### Understanding the Scores\n",
    "- **ROUGE-1**: Measures overlap of individual words (unigrams)\n",
    "- **ROUGE-L**: Measures the longest common subsequence of words\n",
    "\n",
    "Higher scores (closer to 1.0) indicate better lexical similarity.\n",
    "\n",
    "> **üìù Note:** Here we compare two sentences with similar words but different structures using the `evaluate` library. The ROUGE metric provides multiple scores to capture different aspects of text overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698a9cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prediction and a reference text\n",
    "pred = \"the quick brown fox\"\n",
    "label = \"the fox is quick and brown\"\n",
    "\n",
    "# Load the ROUGE metric from the 'evaluate' library\n",
    "rouge = load(\"rouge\")\n",
    "\n",
    "# Compute the scores\n",
    "results = rouge.compute(predictions=[pred], references=[label])\n",
    "\n",
    "print(f\"ROUGE-1 Score: {results['rouge1']:.4f}\")\n",
    "print(f\"ROUGE-L Score: {results['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0601132c",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "The high scores indicate **strong lexical similarity** between the two sentences, even though they have different word orders and structures.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Semantic Similarity\n",
    "\n",
    "### Overview\n",
    "Semantic Similarity goes beyond word matching to understand **meaning**. It converts sentences into numerical vectors (embeddings) and measures how similar they are using cosine similarity.\n",
    "\n",
    "### When to Use\n",
    "- ‚úÖ Paraphrase detection\n",
    "- ‚úÖ Question-answer matching\n",
    "- ‚úÖ Duplicate content detection\n",
    "- ‚úÖ Semantic search applications\n",
    "\n",
    "### How It Works\n",
    "1. **Encode**: Convert sentences to high-dimensional vectors using a pre-trained model\n",
    "2. **Compare**: Calculate cosine similarity (range: -1 to 1)\n",
    "3. **Interpret**: Scores close to 1.0 mean very similar meanings\n",
    "\n",
    "> **üìù Note:** We'll load the `all-MiniLM-L6-v2` model, which is optimized for creating sentence embeddings. Then we'll generate embeddings for predictions and labels, calculating cosine similarity for each pair to see how semantically related they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bd1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Define prediction and label sentences\n",
    "labels = [\"A dog is a loyal pet\", \"Cats are independent animals\", \"The sky is blue\"]\n",
    "preds = [\n",
    "    \"Dogs make great companions\",\n",
    "    \"A cat is a solitary creature\",\n",
    "    \"The ocean is vast\",\n",
    "]\n",
    "\n",
    "# 3. Generate embeddings for each list\n",
    "pred_embeddings = model.encode(preds)\n",
    "label_embeddings = model.encode(labels)\n",
    "\n",
    "# 4. Calculate cosine similarity for each pair\n",
    "for i in range(len(preds)):\n",
    "    similarity = np.dot(pred_embeddings[i], label_embeddings[i]) / (\n",
    "        np.linalg.norm(pred_embeddings[i]) * np.linalg.norm(label_embeddings[i])\n",
    "    )\n",
    "    print(\n",
    "        f\"Pair {i + 1}:\\n  Pred:  '{preds[i]}'\\n  Label: '{labels[i]}'\\n  Similarity: {similarity:.4f}\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d37ffa4",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "Notice that sentences about **cats and dogs** have high similarity scores because their meanings are semantically related. In contrast, sentences about **ocean and sky** have low scores despite both being nature-related, because they describe fundamentally different concepts.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Functional Correctness\n",
    "\n",
    "### Overview\n",
    "For code generation tasks, we need to verify if the **generated code actually works**. Functional Correctness evaluates code by running it against a suite of unit tests.\n",
    "\n",
    "### When to Use\n",
    "- ‚úÖ Code generation evaluation\n",
    "- ‚úÖ Programming assistance tools\n",
    "- ‚úÖ Automated coding challenges\n",
    "- ‚úÖ Algorithm implementation verification\n",
    "\n",
    "### How It Works\n",
    "1. Generate code with the model\n",
    "2. Run the code against predefined test cases\n",
    "3. Calculate the proportion of tests that pass\n",
    "4. Score = (Passed Tests / Total Tests)\n",
    "\n",
    "> **üìù Note:** In this example, we test a function that reverses and capitalizes strings. However, it contains a bug: it fails when the input contains digits. We'll run three test cases to demonstrate how functional correctness catches these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11deef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is supposed to reverse and capitalize a string,\n",
    "# but it has a bug: it fails if the string contains a number.\n",
    "def reverse_and_capitalize(s: str) -> str:\n",
    "    \"\"\"Reverse and capitalize a string, with a hidden bug.\"\"\"\n",
    "    if any(char.isdigit() for char in s):\n",
    "        return \"ERROR - CONTAINS DIGITS\"\n",
    "    return s[::-1].upper()\n",
    "\n",
    "# Test cases: one prediction will trigger the bug\n",
    "code_preds = [\"hello\", \"world1\", \"python\"]\n",
    "test_labels = [\"OLLEH\", \"1DLROW\", \"NOHTYP\"]\n",
    "\n",
    "# Run the generated code against the test labels\n",
    "results = []\n",
    "for pred_code, label in zip(code_preds, test_labels):\n",
    "    output = reverse_and_capitalize(pred_code)\n",
    "    print(f\"Input: '{pred_code}' -> Output: '{output}', Expected: '{label}'\")\n",
    "    results.append(output == label)\n",
    "\n",
    "pass_rate = sum(results) / len(results)\n",
    "print(f\"\\nProportion of tests passed: {pass_rate:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a5c3a",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "The function works correctly for **\"hello\"** and **\"python\"** but fails for **\"world1\"** due to the digit check bug. As a result, the pass rate is **2 out of 3** (approximately 67%), clearly identifying the function's limitation.\n",
    "\n",
    "---\n",
    "\n",
    "## üé≤ Pass@k\n",
    "\n",
    "### Overview\n",
    "Pass@k evaluates scenarios where a model generates **k multiple attempts** for a single problem. If **at least one** of these attempts is correct, it counts as a success.\n",
    "\n",
    "### When to Use\n",
    "- ‚úÖ Code generation with multiple solutions\n",
    "- ‚úÖ Creative tasks with multiple valid answers\n",
    "- ‚úÖ Brainstorming applications\n",
    "- ‚úÖ When diversity in outputs is encouraged\n",
    "\n",
    "### How It Works\n",
    "- Generate k samples for one problem\n",
    "- Check if any sample matches the correct answer\n",
    "- Return 1 if at least one is correct, 0 otherwise\n",
    "- Common values: Pass@1, Pass@5, Pass@10\n",
    "\n",
    "> **üìù Note:** We'll simulate a scenario where a model generated 4 possible answers when asked to \"name a primary color.\" Our function checks if the correct answer (\"blue\") is present anywhere in the list of generated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2421754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_at_k(samples: list[str], label: str) -> int:\n",
    "    \"\"\"Return 1 if any sample in the list matches the label, else 0.\"\"\"\n",
    "    return int(any(s == label for s in samples))\n",
    "\n",
    "# The model generated 4 possible answers for \"Name a primary color.\"\n",
    "label = \"blue\"\n",
    "samples = [\"red\", \"yellow\", \"green\", \"blue\"]\n",
    "\n",
    "# Check if any of the 4 samples is correct\n",
    "pass_score = pass_at_k(samples, label)\n",
    "\n",
    "print(f\"Samples: {samples}\")\n",
    "print(f\"Label: {label}\")\n",
    "print(f\"Pass@4 Score: {pass_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e84c81",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "Since the correct answer **\"blue\"** appears in the list of samples, the function returns a score of **1**, indicating a successful Pass@4. This demonstrates that the model succeeded in generating the correct answer within 4 attempts.\n",
    "\n",
    "---\n",
    "\n",
    "## üßë‚Äç‚öñÔ∏è LLM-as-a-Judge\n",
    "\n",
    "### Overview\n",
    "For **complex and subjective** tasks (creativity, helpfulness, tone), we can leverage another powerful LLM to act as an evaluator. The judge LLM receives the prediction, reference answer, and a detailed rubric to provide scored feedback.\n",
    "\n",
    "### When to Use\n",
    "- ‚úÖ Creative writing evaluation\n",
    "- ‚úÖ Subjective quality assessment\n",
    "- ‚úÖ Multi-criteria evaluation\n",
    "- ‚úÖ Tasks without clear right/wrong answers\n",
    "- ‚úÖ Nuanced scoring requirements\n",
    "\n",
    "### How It Works\n",
    "1. Define a clear evaluation rubric\n",
    "2. Provide the judge with: prediction, reference, and rubric\n",
    "3. Judge returns a score with reasoning\n",
    "4. Can scale to multiple criteria and complex scoring\n",
    "\n",
    "> **üìù Note:** We'll define a rubric for scoring animal predictions with three tiers: perfect match (1.0), same biological class (0.5), or different class (0.0). The judge function will evaluate three different test cases demonstrating each scoring scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92108c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our rubric for the judge.\n",
    "RUBRIC = \"\"\"\n",
    "Score 1.0 if the predicted animal is the same as the label.\n",
    "Score 0.5 if the prediction is a different animal but from the same biological class (e.g., both are mammals).\n",
    "Score 0.0 otherwise (e.g., a mammal and a reptile).\n",
    "\"\"\"\n",
    "\n",
    "# ... A mock function `llm_as_judge` is defined here to simulate an LLM's response ...\n",
    "\n",
    "# --- Test Case 1: Perfect Match ---\n",
    "score1 = llm_as_judge(pred=\"Lion\", label=\"Lion\", rubric=RUBRIC)\n",
    "print(f\"--> Final Score: {score1}\\n\")\n",
    "\n",
    "# --- Test Case 2: Same Class ---\n",
    "score2 = llm_as_judge(pred=\"Tiger\", label=\"Lion\", rubric=RUBRIC)\n",
    "print(f\"--> Final Score: {score2}\\n\")\n",
    "\n",
    "# --- Test Case 3: Different Class ---\n",
    "score3 = llm_as_judge(pred=\"Snake\", label=\"Lion\", rubric=RUBRIC)\n",
    "print(f\"--> Final Score: {score3}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f0e3a",
   "metadata": {},
   "source": [
    "### üìä Results Analysis\n",
    "The LLM-as-a-Judge correctly applies the rubric:\n",
    "- **Test Case 1** (Lion vs Lion): Perfect match ‚Üí Score **1.0** ‚úÖ\n",
    "- **Test Case 2** (Tiger vs Lion): Same biological class (both mammals) ‚Üí Score **0.5** ‚ö°\n",
    "- **Test Case 3** (Snake vs Lion): Different classes (reptile vs mammal) ‚Üí Score **0.0** ‚ùå\n",
    "\n",
    "This demonstrates how an LLM can apply nuanced reasoning to complex evaluation tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Conclusion\n",
    "\n",
    "You've now learned **six powerful techniques** for evaluating Generative AI outputs! Each metric serves different purposes:\n",
    "\n",
    "| Metric | Best For | Strictness |\n",
    "|--------|----------|-----------|\n",
    "| Exact Match | Classification tasks | Highest |\n",
    "| ROUGE | Text summarization | Medium |\n",
    "| Semantic Similarity | Paraphrase detection | Medium |\n",
    "| Functional Correctness | Code generation | High |\n",
    "| Pass@k | Multiple attempts | Low |\n",
    "| LLM-as-a-Judge | Subjective tasks | Customizable |\n",
    "\n",
    "Choose the right metric based on your specific use case! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
